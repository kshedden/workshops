{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Jackknife empirical likelihood analysis of inter-study heterogeneity\n",
    "\n",
    "An emerging method for meta-analysis that works well with smaller sample sizes is the method of [jackknife empirical likelihood](https://pubmed.ncbi.nlm.nih.gov/34015509/).  Specifically, we use this approach when our goal is to quantify the extent of heterogeneity among a collection of treatment effects that were reported in different research studies.\n",
    "\n",
    "We begin with the simple meta-analytic model $y_i = \\mu + \\theta_i + s_i\\epsilon_i$, where $y_i$ is an observed (estimated) treatment effect for study $i$, $\\mu$ is the common (average) treatement effect, $\\theta_i$ is the true, unique treatment effect for study $i$, $s_i$ is the (known) standard error for study $i$, and the $\\epsilon_i$ are independent centered and standardized random deviates.  Here we treat the $\\theta_i$ as independent random variables with mean zero and variance $\\tau^2$.  \n",
    "\n",
    "If $\\tau^2$ is zero, there is no heterogeneity.  Values of $\\tau^2$ greater than zero correspond to increasingly heterogeneous treatment effects.  Note that $\\tau^2$ is not a standardized measure of heterogeneity, like $\\tau^2 / (\\tau^2 + {\\rm Avg}(s_i^2))$. \n",
    "\n",
    "We can unbiasedly estimate $\\tau^2$ using $\\hat{\\tau}^2 = \\hat{\\rm var}(y_1, \\ldots, y_n) - {\\rm Avg}(s_i^2)$.  This is a simple estimate to compute, but it is not straightforward to assess it inferentially.  That is, it is difficult to assess how precisely we have estimated $\\tau^2$, and whether it is plausible that $\\tau^2 = 0$.\n",
    "\n",
    "To apply jackknife empirical likelihood here, we construct jackknife values $\\hat{\\tau}_{-i}$ by deleting observation (study) $i$ and recalculating $\\hat{\\tau}^2$.  We then construct _pseudo-observations_ $\\eta_i = n\\hat{\\tau}^2 - (n-1)\\hat{\\tau}^2_{-i}$.  Next, we construct a discrete probability distibution on the pseudo-observagtions $\\eta_i$ by assigning probability $p_i$ to point $\\eta_i$, where $p_i \\ge 0$ and $\\sum_i p_i = 1$ are imposed.  In addition, we impose the constraint $\\sum_i \\eta_i p_i = \\tau_0^2$, where $\\tau_0^2$ is a provisional value of $\\tau^2$.  That is, we force the expected value of the empirical likelihood to be exactly equal to $\\tau_0^2$.  Subject to these constraints we optimize $\\sum_i \\log p_i$ which can be seen as a form of nonparametric maximum likelihood.  The value $L(\\tau_0^2)$ of $\\sum_i \\log p_i$ for a given provisional value $\\tau_0^2$ is the empirical likelihood at $\\tau_0^2$.  We can treat $L$ like a conventional likelihood, so that if $\\tau^{2*}$ optimizes $L$, then $2(L(\\tau^{2*}) - L(\\tau_0^2))$ is a $\\chi^2_1$ deviate which tests the hypothesis that the true value of $\\tau^2$ is equal to $\\tau_0^2$.  The values of $\\tau_0^2$ for which the null hypothesis cannot be rejected at level $\\alpha$ is a $100 \\times (1 - \\alpha)\\%$ confidence interval for $\\tau^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats.distributions as dist\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "In the cell below, we write a function that simulated data that could be used for a (mock) meta-analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_study_dat(n_study, pes, arm_size_mean, arm_size_cv, arm_size_cor, var_cv, clust_icc):\n",
    "    \"\"\"\n",
    "    Simulate data for meta-analysis.  Each study in the meta-analysis is a two arm-study.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_study : number of studies\n",
    "    pes : population effect size (can be scalar for homogeneous or vector for heterogeneous studies)\n",
    "    arm_size_mean : the expected sample size of one study arm\n",
    "    arm_size_cv : the coefficient of variation of study arm sizes\n",
    "    arm_size_cor : the correlation between effect sizes of the two arms (on copula scale)\n",
    "    var_cv : the coeffient of variation of the unexplained variance\n",
    "    clust_icc : if clust_icc, construct 5 study clusters such that there is correlation within the clusters,\n",
    "                the cluster assignments are returned as 'clust'\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    md : estimated treatment effects for each study\n",
    "    sig : estimated (residual) standard deviation for each study\n",
    "    N1 : sample size for arm 1 in each study\n",
    "    N2 : sample size for arm 2 in each study\n",
    "    clust : indicators of study clusters, or None if no clustering is present\n",
    "  \n",
    "    Notes\n",
    "    -----\n",
    "    The unexplained variance always has mean 1.\n",
    "    \"\"\"\n",
    "    # Generate sample sizes for two arms in each study using a Gaussian copula\n",
    "    z = rng.normal(size=(n_study, 2))\n",
    "    z[:, 1] = arm_size_cor*z[:, 0] + np.sqrt(1-arm_size_cor**2)*z[:, 1]\n",
    "    u = dist.norm.cdf(z)\n",
    "    v = (arm_size_mean * arm_size_cv)**2\n",
    "    a = arm_size_mean**2 / v\n",
    "    b = v / arm_size_mean\n",
    "    N = dist.gamma(a, scale=b).ppf(u)\n",
    "    N = np.ceil(N).astype(int)\n",
    "    N1 = N[:, 0]\n",
    "    N2 = N[:, 1]\n",
    "    \n",
    "    # Now generate standard deviations, centered at 1\n",
    "    v = var_cv**2\n",
    "    sig = rng.gamma(1/v, scale=v, size=n_study)\n",
    "    \n",
    "    # Convert the standard deviations to standard errors\n",
    "    f = (N1 + N2) / (N1 * N2)\n",
    "    se = np.sqrt(sig**2 * f)\n",
    "    \n",
    "    z = rng.normal(size=n_study)\n",
    "    if clust_icc == 0:\n",
    "        clust = None\n",
    "    else:\n",
    "        clust = rng.choice(range(5), n_study)\n",
    "        for i in range(5):\n",
    "            jj = np.flatnonzero(clust == i)\n",
    "            if len(jj) > 0:\n",
    "                z[jj] = np.sqrt(clust_icc)*rng.normal() + np.sqrt(1 - clust_icc)*z[jj]\n",
    "    md = pes + z*se\n",
    "    \n",
    "    return md, sig, N1, N2, clust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The jackknife empirical likelihood approach described above is implemented in the following two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cochran_het_pseudo(md, se):\n",
    "    \"\"\"\n",
    "    Calculate jackknife pseudo-observations for the heterogeneity statistic\n",
    "    (an unbiased estimate of the variance of study treatment effects) given\n",
    "    a collection of studies with estimated treatment effects 'md' and standard \n",
    "    errors 'se'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The heterogeneity statistic\n",
    "    hetf = lambda md, se: md.var() - (se**2).mean()\n",
    "    \n",
    "    h0 = hetf(md, se)\n",
    "    n = len(md)\n",
    "    jack = np.zeros(n)\n",
    "    for j in range(n):\n",
    "        ii = [i for i in range(n) if i != j]\n",
    "        h1 = hetf(md[ii], se[ii])\n",
    "        jack[j] = n*h0 - (n-1)*h1\n",
    "    return jack\n",
    "\n",
    "def cochran_het_jel(md, se, npt=25):\n",
    "    \"\"\"\n",
    "    Given a collection of studies with estimated treatment effects 'md' and standard\n",
    "    errors 'se', construct a grid of 'npt' points for the plausible values of the \n",
    "    heterogeneity statistic, then calculate empirical log likelihoods for the points \n",
    "    on this grid.\n",
    "    \"\"\"\n",
    "    n = len(md)\n",
    "    jack = cochran_het_pseudo(md, se)\n",
    "    bounds = ((0, 1) for _ in range(n))\n",
    "    A = np.vstack((np.ones(n), jack))\n",
    "\n",
    "    # Estimate the profile empirical likelihood at these points\n",
    "    hgrid = np.linspace(jack.min()+1e-4, jack.max()-1e-4, npt)\n",
    "\n",
    "    # The log-likelihood that we are optimizing.\n",
    "    f = lambda p: -np.log(np.clip(p, 1e-4, 1)).sum()\n",
    "    \n",
    "    # Starting values for the optimization\n",
    "    start = np.ones(n) / n\n",
    "\n",
    "    def llf(qs):\n",
    "        rhs = np.r_[1, qs]\n",
    "        lc = LinearConstraint(A, rhs, rhs)\n",
    "        mr = minimize(f, start, method=\"SLSQP\", bounds=bounds, \n",
    "                      constraints=lc, options={\"maxiter\": 5000})\n",
    "        return mr\n",
    "\n",
    "    ll = np.zeros(npt)\n",
    "    for j in range(npt):\n",
    "        bounds = ((0, 1) for _ in range(n)) # needs to be re-created each time\n",
    "        mr = llf(hgrid[j])\n",
    "        if not mr.success:\n",
    "            # Raise an exception if the optimization fails.\n",
    "            print(\"failed: \", hgrid[j])\n",
    "            print(mr)\n",
    "            1/0\n",
    "        ll[j] = -mr.fun\n",
    "\n",
    "    ll -= ll.max()\n",
    "    return ll, hgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "To test the procedure, we first generate test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50 # number of studies\n",
    "\n",
    "# The true effect sizes in the n studies.  The target of inference is the population\n",
    "# variance of the values in 'es' (which we call \"tau^2\").\n",
    "es = rng.choice([0, 1], size=n)\n",
    "\n",
    "md, sig, N1, N2, clust = gen_study_dat(n, es, 30, 0.5, 0.7, 0.6, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Next we plot the \"profile empirical likellihood function\" which reflects whether various potential values of $\\tau^2$ can plausibly fit the data.  The higher the profile likelihood, the more plausible the value.  The interval of $\\tau^2$ values corresponding to a 95% confidence interval is shown in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(md)\n",
    "f = (N1 + N2) / (N1 * N2)\n",
    "se = sig * np.sqrt(f)\n",
    "ll, qgrid = cochran_het_jel(md, se)\n",
    "\n",
    "tr = dist.chi2(1).ppf(0.95)\n",
    "\n",
    "ii = np.flatnonzero(qgrid >= 0)\n",
    "qgrid = qgrid[ii]\n",
    "ll = ll[ii]\n",
    "\n",
    "plt.plot(qgrid, ll)\n",
    "ii = ll > -tr\n",
    "plt.plot(qgrid[ii], ll[ii], color=\"red\")\n",
    "plt.ylabel(\"Profile empirical likelihood\")\n",
    "plt.xlabel(r\"$\\tau^2$\")\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
