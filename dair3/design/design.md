# Rigorous statistical design

- What is the goal of conducting
  [research](https://en.wikipedia.org/wiki/Research) using the
  [scientific method](https://en.wikipedia.org/wiki/Scientific_method)?

  - To gain insight into the the mechanisms underlying the natural world.

  - To describe and characterize the state of the natural world.

  - To produce new scientific
    [knowledge](https://en.wikipedia.org/wiki/Knowledge).

  - To test theories about mechanisms that explain the natural world.

  - To produce new concepts, methods, and theories.

- How (at a very high level) should we go about the process of conducting
  research?

- There is much deep and fundamental work in the
  [philosophy of science](https://en.wikipedia.org/wiki/Philosophy_of_science).
  We will only cover this topic briefly and superficially here, in favor of more
  pragmatic topics.

  - For those interested, here is a readable account of the work of the very
    influential philosopher of science
    [Karl Popper](https://iep.utm.edu/pop-sci).

- At a very high level, most research is either
  [inductive](https://en.wikipedia.org/wiki/Inductive_reasoning) or
  [deductive](https://en.wikipedia.org/wiki/Deductive_reasoning). Inductive
  research is data-driven and empirical, that is our focus here. Deductive
  research starts with "first principles", and derives new findings logically
  from them.

  - Many researchers combine data-driven and first principles approaches in
    their work.

- Empirical research generally involves design (planning), data collection, and
  data analysis.

  - Design refers to the process of developing a rationale and goals for the
    research, stating (falsifiable)
    [hypotheses](https://en.wikipedia.org/wiki/Hypothesis), and documenting how
    the data will be collected, analyzed, and interpreted.

  - Data collection refers to the process of making and recording measurements.

  - Data analysis refers to the execution of a data analysis plan, which
    includes carrying out the analysis, interpreting the results, and formally
    assessing the [uncertainty](https://en.wikipedia.org/wiki/Uncertainty) of
    all findings.

- Our focus here is on learning about entities that exist with some form of
  replication. For example, if we are interested in a certain type of human
  being, there are generally many "replicates" of such a person that we can
  potentially study. In this case, a human is the
  [unit](https://en.wikipedia.org/wiki/Unit_of_analysis) (or _unit of
  analysis_). The set of all humans of interest is the
  [population](https://en.wikipedia.org/wiki/Statistical_population) and the
  specific humans that we observe and (potentially) manipulate is the
  [sample](<https://en.wikipedia.org/wiki/Sampling_(statistics)>)).

- In some areas of research, replication is challenging or impossible.  For
  example, in research on the Earth's climate history, "treatments" such as
  changes to greenhouse gas emissions are essentially historical "one-offs".
  In public policy, singular events like changes to federal laws are similarly
  non-replicable.  At best we can look for "pseudo-replicates" when somewhat
  similar events happened at another historical moment.

- There are many different types of research "studies". We are not aiming here
  to produce a typology for all possible situations. Here are some of the major
  features that differentiate designs of research studies.

  - In an _interventional study_, the units are manipulated by the researcher,
    treatment assignments are
    [exogenous](https://en.wikipedia.org/wiki/Exogeny). This is also sometimes
    called an [experiment](https://en.wikipedia.org/wiki/Experiment).

  - In an
    [observational study](https://en.wikipedia.org/wiki/Observational_study),
    the experimental units are not manipulated by the researcher. Any
    differences that form the basis for comparison arise naturalistically and
    are subject to [confounding](https://en.wikipedia.org/wiki/Confounding).

  - A _controlled_ study involves a comparison to a group (arm) that is
    untreated, or that is treated with a conventional well-understood treatment
    (e.g. "standard of care" in clinical research).

  - A
    [randomized controlled study](https://en.wikipedia.org/wiki/Randomized_controlled_trial)
    (or "trial", RCT) is a controlled study in which the subjects are assigned
    to the treatment or control group by randomization.

  - In a [longitudinal study](https://en.wikipedia.org/wiki/Longitudinal_study),
    each unit is observed on multiple occasions. This is also known as a _panel
    study_.

  - In a
    [cross-sectional study](https://en.wikipedia.org/wiki/Cross-sectional_study),
    each unit is observed on a single occasion.

  - In a [cohort study](https://en.wikipedia.org/wiki/Cohort_study), a group of
    units are followed over time. The units are initially either similar, or
    differ primarily with respect to a specific factor of interest (but they are
    not selected based on this factor). Exposures happen naturalistically and
    we can assess at the end of the study which exposures occurred and what
    outcomes followed the occurrence of these exposures.

    - A
      [prospective cohort study](https://en.wikipedia.org/wiki/Prospective_cohort_study)
      involves collecting data as the events of interest occur in real time.

    - A
      [retrospective cohort study](https://en.wikipedia.org/wiki/Retrospective_cohort_study)
      involves data that were collected for purposes other than your research
      study (i.e. for other research studies or for administrative purposes).

  - In a [case/control study](c) units are deliberately selected at the outset
    to belong to contrasting states of a factor of interest.

  - A _micro-longitudinal_ study is a study with relatively few units, but each
    unit is measured intensively on multiple characteristics over time.

  - An
    [adaptive interventional study](<https://en.wikipedia.org/wiki/Adaptive_design_(medicine)>)
    is one in which the treatment assignments depend on the results of
    previously collected data within the same study, based either on an _interim
    analysis_ or on _continual reassessment_ of findings. In some cases the
    design updates only impact patients not yet enrolled, while in other cases
    the design updates impact the people currently undergoing treatment (e.g. in
    a [SMART](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4167891) design).

## Measurement

- Some quantities can be directly
  [measured](https://en.wikipedia.org/wiki/Measurement) in a more or less exact
  manner, like a person's height.

- Some quantities are difficult to measure because they change rapidly (heart
  rate) or differ based on who is doing the measurement or how the measurement
  is taken (blood pressure).

- Some quantities are difficult to measure because they place a lot of burden on
  the subjects (invasive diagnostic tests).

- Some measurements require human subjects to report behaviors, for example food
  intake, sleep, or substance use. These are burdensome and often biased and
  imprecise. They are subject to
  [recall bias](https://en.wikipedia.org/wiki/Recall_bias) and
  [social desirability bias](https://en.wikipedia.org/wiki/Social-desirability_bias),
  among other issues.

- Technology is rapidly transforming the landscape for measurements of human
  behaviors ([wearables](https://en.wikipedia.org/wiki/Wearable_technology) for
  activity and sleep, real time monitors for blood glucose, bio-assays for
  nicotine use).

- Some quantities are difficult to measure because they are abstract and there
  is debate about exactly what the quantity means. The term
  [construct](<https://en.wikipedia.org/wiki/Construct_(psychology)>) is used in
  many cases to refer to things like social attitudes, or skills (e.g. language
  or math skills) that cannot be simply and directly measured.

  - A _scale_ is a quantitative value that captures the state of a construct in
    a single unit. It is usually formed by aggregating several _items_ that can
    be measured directly.

  - There is a branch of
    [psychometrics](https://en.wikipedia.org/wiki/Psychometrics) that deals with
    methods for forming scales out of item-level (scalar) variables.

  - Statistical methods such as
    [factor analysis](https://en.wikipedia.org/wiki/Factor_analysis) can be used
    to produce scales from a collection of items, and assess to
    _dimensionality_, i.e. whether there exists only one versus multiple
    independent constructs underlying a collection of measured items. Scales
    have been developed for numerous traits such as depression, overall physical
    health, hopefulness, and political ideology.

  - The [Delphi method](https://en.wikipedia.org/wiki/Delphi_method) is used to
    objectively produce items that can be used to construct novel scales.

- Most values that are measured are measured with some level of
  [error](https://en.wikipedia.org/wiki/Observational_error). Measurement errors
  can be classified into two broad types.

  - _Systematic errors_ reflect bias in the data. These are errors that occur
    persistently in one direction upon repeated sampling. For example, in the
    context of measuring blood pressure, there is a well-known systematic error
    known as "white coat hypertension" in which some people's blood pressure
    rises due to the stress of interacting with the person doing the
    measurement.

  - _Random errors_ are a type of error that is transient or specific to one
    instance of making a measurement. Using blood pressure again as an example,
    there are substantial random errors due to the skill required to accurately
    detect a pulse as the blood pressure cuff is depressurized. The variance of
    these random errors may depend on the skill of the person doing the
    measurement.

  - Random errors can further be classified as classical errors and
    [Berkson errors](https://en.wikipedia.org/wiki/Berkson_error_model).

    - Classical errors arise due to additive random measurement error that is
      independent of the true value. Classical errors arise from the error model
      $X_{\rm obs} = X_{\rm true} + \eta$, where $E[\eta | X_{\rm true}] = 0$,
      with $\eta$ being the random measurement error.

    - Berkson errors arise when $E[\eta | X_{\rm obs}] = 0$. A typical example
      is where we wish to study the yield of a chemical reaction at a specific
      temperature. We set the temperature to a desired level $T$ and use a
      heating device to produce the desired temperature. Since the heating
      device can never be perfect, the actual temperature will arguably follow
      $X_{\rm true} = X_{\rm obs} + \eta$, where $\eta$ is measurement error
      satisfying $E[\eta | X_{\rm obs}] = 0$.

## Causality and confounding

- Most research aims to understand relationships among factors. In health
  research we often speak of _exposures_ and _outcomes_, or _treatments_ and
  outcomes. The term "exposure" is typically used when the factor occurs
  naturalistically whereas "treatment" is used for interventions or experimental
  manipulations.

- If $X$ is an exposure and $Y$ is an outcome, there is an _association_ between
  $X$ and $Y$ if $X$ and $Y$ are not statistically
  [independent](<https://en.wikipedia.org/wiki/Independence_(probability_theory)>)).
  A [causal](https://en.wikipedia.org/wiki/Causality) association is one in
  which manipulation of $X$ would lead to a change in $Y$ (on average or in some
  statistical sense). One of the fundamental tenets of empirical research is
  that
  [correlation is not causation](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation).

- A [spurious relationship](https://en.wikipedia.org/wiki/Spurious_relationship)
  is a non-causal associations. Below are some examples of (presumably) spurious
  relationships.

  - More ice cream is consumed on days in which the number of drowning deaths is
    greater. If we were to ban the consumption of ice cream, would we see a
    reduction in drowning deaths?

  - From 2000-2020, the marriage rate was lower in years when internet usage was
    greater. If the internet had not been invented, would marriage rates have
    followed a different pattern?

- Why are some associations not causal?

  - The main explanation for associations being non-causal is the role of a
    _confounding variable_ (or "confounder"). This is a variable that causally
    influences both the exposure and the outcome. Let's reconsider the two
    examples above:

    - Temperature and rainfall are likely confounding factors in the
      relationship between ice cream consumption and drowning. On warm days with
      little rain, people enjoy eating ice cream and these are the same
      conditions that might lead people to go to the pool or the beach. When
      there are more people at the pool or beach, there is a greater risk for
      drowning deaths occurring.

    - [Secular trends](https://en.wikipedia.org/wiki/Secular_variation) relating
      to technological and social change are very likely acting as confounders
      in the relationship between internet usage and marriage rates.

  - A confounder can be either known or unknown, and can be either measured or
    unmeasured.

  - Unfortunately there is no automatic process for identifying confounders, and
    there is rarely any way to know whether all confounders of a given
    relationship have been identified.

  - At a high level, when designing a study, one or more of the following
    strategies are usually employed to mitigate the risks of confounding. They
    are listed in decreasing order of rigor:

    - Randomization is usually the best way to limit or eliminate the risk of
      confounding. If the treatment is assigned at random, it is impossible for
      it to be causally influenced by any factor (regardless of whether the
      factor is known or unknown, measured or unmeasured).

    - If randomization is impossible, the next best approach is usually to
      achieve _balance_ of all measured confounders between the arms.

    - If we cannot achieve balance then various statistical adjustments are made
      in a _post-hoc_ manner.

- Another role for a "third variable" is as a _collider_. A collider is a
  variable that is causally influenced by both the exposure and an outcome.
  Colliders are not confounders and in fact controlling for a collider
  introduces _collider stratification bias_. If a variable is a collider it
  should generally be ignored in the analysis.

- Formal analysis of confounding often takes place in the context of the
  [Neyman-Rubin causal model](https://en.wikipedia.org/wiki/Rubin_causal_model),
  which posits the existence of
  [potential outcomes](https://en.wikipedia.org/wiki/Counterfactual_conditional).
  In brief, every unit has potential outcomes under every possible treatment. We
  only observe one of these potential outcomes, along with the indicator of
  which outcome we observe. This indicator is endogenous in most cases.

## Randomization

- In research in which units are assigned to treatment arms, assigning the units
  [randomly](https://en.wikipedia.org/wiki/Randomization) guarantees that there
  is no confounding at the population level. That is, there can be no
  statistical dependence between the assigned treatments and characteristics of
  the units.

- _Balance_ refers to the extent to which covariates have equal distributions
  across treatment arms within the study data (the sample). Confounding will
  generally lead to a lack of balance, but even if there is no confounding,
  there can be a lack of balance, especially when the sample size is small. The
  is because randomization (which guarantees no confounding) produces balance on
  average, but in any given study randomization will usually not produce perfect
  balance.

- Bias can result from lack of balance due to chance, even in the absence of
  confounding.

- For measured (potential) confounders, we can quantify and compensate for any
  lack of balance. For unmeasured or unknown confounders, randomization is
  usually the only practical way to avoid bias due to confounding.

- The most basic type of randomization is simple randomization, where each unit
  is independently assigned to a treatment group, either with uniform
  probabilities (equal probability of assignment to each arm) or with
  probabilities that are pre-determined to achieve desired relative group sizes
  (e.g. assigning to treatment with twice the probability of assigning to
  control).

- Numerous approaches to randomization have been devised that aim to preserve
  the benefits of randomization, while also producing balance with respect to
  observed confounders. Some of these approaches are:

  - In
    [stratified randomization](https://en.wikipedia.org/wiki/Stratified_randomization)
    the units are partitioned into more homogeneous groups. For example, in a
    study of a treatment for kidney disease we may partition the population into
    people with and without hypertension. Say we plan to recruit 60 people with
    hypertension and 30 people without hypertension. In 1:1 stratified
    randomization we would assign exactly 30 of the people with hypertension to
    the treatment arm, and exactly 15 of the people without hypertension to the
    treatment arm.

  - _Minimization_ is a class of methods that addresses the practical issue that
    in many research studies, subjects are recruited over time, and we do not
    have a listing of the subjects and their measured confounders at the outset
    of the study. For this reason, we cannot use stratified randomization.
    Minimization uses a "biased coin" to allocate units to study arms in a way
    that corrects for biases that arise during subject recruitment.

- A concern in studies where units are recruited sequentially is bias on the
  part of the research team in any decisions that could influence recruitment of
  subjects. The research team has an incentive to exclude subjects who would be
  assigned to the active treatment arm if those students seem likely to do
  poorly. Analogously, there is an incentive to exclude subjects who will be
  assigned to the control arm if they appear likely to do well. Maintaining some
  degree of randomness in the assignments helps to mitigate this issue.

## Foundations of statistical inference

- Most statistical data analysis is based on probability modeling. That is, we
  posit that our data are a random sample from a probability distribution
  $P_\theta$, where $\theta$ is a
  [parameter](https://en.wikipedia.org/wiki/Statistical_parameter) that captures
  aspects of the scientific research question.

- A probability model should capture all _sources of variation_ influencing the
  data.

- _Probability theory_ considers the properties of a sample $D$ from a given
  probability model $P$. _Statistical inference_ is the reverse of this - given
  a random sample $D$, what can we say about the probability distribution $P$
  from which $D$ was sampled?

- Statistical inference generally begins with
  [estimation](https://en.wikipedia.org/wiki/Estimation). Formally, this involves
  devising a function $\hat{\theta}(D)$, where $D$ is the _observed data_, such
  that $\hat{\theta}(D)$ is likely to be close to the true parameter value
  $\theta$. This function is called an _estimator_.

- It is common to refer to $P_\theta$ as the
  [population](https://en.wikipedia.org/wiki/Statistical_population), $D$ as the
  [sample](<https://en.wikipedia.org/wiki/Sampling_(statistics)>), and
  $\hat{\theta}$ as the _parameter estimate_.

- There are many ways to obtain parameter estimates, two of the most common are
  the
  [method of moments](<https://en.wikipedia.org/wiki/Method_of_moments_(statistics)>),
  and [maximum likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood)
  analysis.

- Statistical estimators may exhibit desirable properties, including
  [unbiasedness](<https://en.wikipedia.org/wiki/Bias_(statistics)>),
  [consistency](<https://en.wikipedia.org/wiki/Consistency_(statistics)>), and
  [efficiency](<https://en.wikipedia.org/wiki/Efficiency_(statistics)>).

- The [standard error](https://en.wikipedia.org/wiki/Standard_error) is a key
  tool for characterizing the precision or uncertainty in a parameter estimate.
  It is the standard deviation of the
  [sampling distribution](https://en.wikipedia.org/wiki/Sampling_distribution)
  of the random variable $\hat{\theta}$, which is induced by the underlying
  distribution of the data, $P(D)$.

- If the sampling distribution of $\hat{\theta}$ is approximately
  [Gaussian](https://en.wikipedia.org/wiki/Normal_distribution), then the
  standard error is all one needs to fully characterize the estimation errors of
  an unbiased estimator. Due to the
  [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem),
  many estimators behave approximately like Gaussian random variables. In cases
  that this does not hold, with additional work an approximation to the
  non-Gaussian sampling distribution can usually be obtained.

- Many research questions cannot be reduced to a single scalar parameter, so the
  parameter of interest is often a vector. Vector-valued parameters can often be
  partitioned into parameters of primary interest and
  [nuisance parameters](https://en.wikipedia.org/wiki/Nuisance_parameter).

## Statistical power

- [Statistical power](https://en.wikipedia.org/wiki/Statistical_power) is a
  measure of how likely a study is to yield a positive finding, if a positive
  finding is the true state of the system being studied. Usually this refers to
  the
  [power of a hypothesis test](https://en.wikipedia.org/wiki/Power_of_a_test),
  referring to the probability of rejecting the null hypothesis when the null
  hypothesis is false.

- Statistical power says nothing about what happens when the null hypothesis is
  true (that is the
  [level](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors)) of a test.

- The power says nothing about the possible biases in a study. For example, we
  could have high power to reject the null hypothesis in a situation where, due
  to bias, rejecting the null does not reflect the claimed level of evidence.

- A broader definition of statistical power includes other measures of study
  success, such as achieving a smaller standard error, a narrower confidence
  interval, or more accurate predictions.

- Power analysis is strongly linked to the notion of
  [effect size](https://en.wikipedia.org/wiki/Effect_size), which quantifies the
  strength of an effect relative to all sources of variation. A basic example of
  an effect size is _Cohen's D_, which can be defined in a two-group comparison
  as $(E[X] - E[Y]) / s_p$, where $s_p^2$ is a pooled variance such as
  $({\rm Var}[X] + {\rm Var}[Y])/2$.

  - A key property of an effect size is that it is independent of the sample
    size.

- Power analysis can be conducted based on a stated design for data collection
  and data analysis. Typically this involves one of the following:

  - Find the sample size necessary to have a defined power (usually 80%) to
    detect a given effect size.

  - Find the effect size that can be detected with a given power (usually 80%),
    for a given sample size.

- Using statistical theory, it is possible to develop a detailed understanding
  of the factors that influence the power in any given setting. Often this comes
  down to assessment of the standard error for an estimator of interest.

- As a basic example, the standard error of the mean is $\sigma/\sqrt{n}$, where
  $\sigma^2$ is the variance and $n$ is the sample size. Based on this
  expression, we know that the only factors that determine the standard error
  for mean estimation are the sample size and the variance, and that they
  combine as a specific rational function.

- Some of the factors that typically influence power are:

  - Larger sample sizes almost always lead to greater power. In a multi-arm
    study, there are multiple sample sizes. While greater sample sizes in any
    group corresponds to improved power, the sample size of the smallest group
    generally has the greatest impact on power.

  - Greater scatter in the data (residual variance) almost invariably decreases
    power. Often, group-wise sample size and variance are inversely related to
    power. That is, if the treatment group has sample size $n_t$ and residual
    variance $\sigma_t^2$, the power for many tests is related to the ratio
    $n_t/\sigma_t^2$. Doubling the sample size or reducing the residual variance
    by a factor of two have equal impacts on the power.

  - In many regression-type analyses,
    [collinearity](https://en.wikipedia.org/wiki/Multicollinearity) is a major
    determinant of power. A typical example of collinearity can arise when we
    wish to control for confounder $Z$ while assessing the effect of a treatment
    variable $X$. In an observational study, $X$ and $Z$ can be strongly
    correlated. For example, suppose we wish to study the association of alcohol
    consumption ($X$) with an outcome $Y$, adjusting for tobacco use ($Z$). In
    many populations, alcohol and tobacco use will be correlated. This leads to
    reduced power when controlling for the confounder.

## Experimental design

- _Experiments_ are studies in which all interventions are assigned to units by
  the researcher. Unlike in most observational studies, units do not select
  their own treatments (either explicitly or implicitly).

- In some experiments, treatments are assigned at random, but if units are
  heterogeneous it is usually advisable to explicitly assign the treatments in a
  way that balances with respect to this heterogeneity.

- A common setting is when the experimental units are clustered into _blocks_ of
  a limited size. The classic example of a block is a plot of land in an
  agricultural field experiment. The characteristics of every plot cannot be
  perfectly controlled (e.g. in terms of sunlight, rainfall, soil conditions,
  etc.). To address this, each plot can be divided in half, and the two halves
  are randomly assigned, one to the treatment and one to the control.

- In current medical research, many factors arise that could serve as blocking
  factors, such as batches of reagents, litters of animals, plates or batches of
  assays, technician effects, and drifting instrument calibration.

## Surveys

- A _sample survey_ is a research tool in which the goal is to quantify the
  state of a population, with a primary focus on achieving low bias for a
  defined target population.

- The term "survey" usually should refer to any overall research effort that
  aims to characterize a population unbiasedly. This could refer to a study with
  human subjects but could also refer to surveying, say, the distribution of
  tree species in a geographic area. In the case of surveys that involve
  interviewing human subjects, a _questionnaire_, _survey form_, or _survey
  instrument_ refers to the actual assessment items that are used for each
  unit.

- Unlike most trials and studies (cohort, case/control, etc.) the goal of a
  survey is not usually to assess the effect of an exposure or intervention, and
  a survey does not usually have "arms" corresponding to different treatments or
  exposures.

- Most sample surveys use random selection to identify subjects. The resulting
  sample is called a _probability sample_.

- A special type of survey is a [census](https://en.wikipedia.org/wiki/Census),
  which aims to measure the entire population, rather than measuring only a
  sample of a population.

  - In most cases a census is either impossible or impractical. A well-conducted
    sample survey can give results that are for all practical purposes just as
    informative as a census, usually at a much lower expense.

## Sampling

- In surveys, as well as in some other contexts, it is important to carefully
  sample units from a population in such a way that unbiased and precise results
  can be obtained from the sample.

- The most basic type of sampling is a
  [simple random sample (SRS)](https://en.wikipedia.org/wiki/Simple_random_sample),
  which is a sample of size $k$ from a population of size $n$ in which any
  subset of size $k$ is equally likely to be selected.

- An SRS can be obtained if we have a
  [sampling frame](https://en.wikipedia.org/wiki/Sampling_frame) which is a
  complete list of all units in the population. For example, if we want to
  sample the employees of a company or the students enrolled in a school, a
  sampling frame would generally be available and it would be practical to
  obtain a simple random sample from it.

  - For larger or more intangible populations, like the total population of a
    geographic region, a sampling frame is usually not available. Even if a
    sampling frame is available, it may be difficult to reach certain units, or
    (in the case of human subjects) people may decline to participate.

- A common type of survey is a _cluster sample_. In a cluster sample, the
  population is partitioned into many _primary sampling units_ (PSU), which
  often correspond to geographic areas. Then, a limited number of PSUs are
  selected at random (possibly with probabilities proportional to size), and
  units are selected from the PSUs using simple random sampling (or something
  that approximates it). In some cases there are two or more levels, e.g.
  randomly select cities, then randomly select schools within cities, then
  randomly select students within schools.

- In a _stratified sample_, the population is partitioned into groups, e.g.
  people are stratified by race, and separate samples are drawn for each
  stratum. In this way, the sample size per stratum is fixed rather than random,
  which would be the case if a simple random sample were obtained. A rationale
  for stratification is to guarantee coverage of people from all groups of
  interest, including groups that are relative small.

- In some cases sampling probabilities are adjusted to increase statistical
  power for comparisons of interest, even if this produces a sample that is not
  representative of the overall population. For example, suppose that one of the
  research goals is to compare outcomes between Black and White subpopulations,
  in a setting where the White population is, say, 3 times greater than the
  Black population. One possibility would be to sample the same number of Black
  and White subjects, which would maximize statistical power for comparisons
  between these two races (if the variances within the two races are equal -- if
  the variances are unequal we would want $n_b/\sigma_b^2 = n_w/\sigma_w^2$,
  where $n_b$ and $n_w$ are the Black and White sample sizes and $\sigma_b^2$
  and $\sigma_w^2$ are the response variances for Black and White subjects).

- If sampling weights are used to maximize power for inter-group comparisons,
  weighting at the analysis level can be used to unbiasedly estimate population
  parameters. If we, say, over-sample Black compared to White subjects at a rate
  of 3 to 1, then when estimating population parameters (not race-specific
  parameters), we would weight the White respondents 3 times more than the Black
  respondents to compensate for the biased sampling.

- Another role for sampling weights is to compensate for non-response (a form of
  missing data). Suppose that subjects were sampled in a representative manner,
  but males were responded at 4/5 the rate of females. In this case we would
  weight male responses by a factor of 5/4 to compensate for the lower response
  rate.

## Missing data and selection bias

- [Missing data](https://en.wikipedia.org/wiki/Missing_data) refers to data that
  exist but are not known to the researcher.

- [Truncation](<https://en.wikipedia.org/wiki/Truncation_(statistics)>) and
  [censoring](<https://en.wikipedia.org/wiki/Censoring_(statistics)>) arise in
  studies in which we are concerned with the times at which one or more events
  occur.

  - We will focus on one type of truncation called _left truncation_ here. Left
    truncation refers to a form of selection bias in which an observation cannot
    be made unless the event of interest occurs after a _truncation time_. For
    example, if we are considering death due to a specific disease, using
    records from a particular health care system, we can never observe people
    who died of the disease without enrolling in the health care system. The age
    at which a person enrolls in the system is thus a (left) truncation time.

  - Right censoring refers to a form of _partially observed data_ in which we
    know that an event of interest did not occur before a specific time, but we
    do not know when the event occurred (if ever). Right censoring is very common
    in health studies where some units in the sample have not yet had an event
    at a particular time, e.g. the time when the data were obtained for
    analysis.

  - Left censoring is less common than right censoring, but it can occur, for
    example, if a measurement falls below a "limit of detection" (e.g. the
    concentration of a chemical in a blood sample is below the limit of
    detection for the assay used to assess the chemical).

- Types of missing data

  - Let $I$ be the indicators of which data are observed, and let $X$ denote all
    data (both observed and unobserved). $X[I]$ is therefore the observed data
    and $X[I^c]$ is the unobserved data.

  - _Missing completely at random (MCAR)_ means that $I$ is independent of $X$.

  - _Missing at random (MAR)_ means that $I$ is independent of $X[I^c]$ given
    $X[I]$.

  - _Missing not at random (MNAR)_ refers to all situations that are not MAR.

- Approaches to handling missing data

  - _Complete case analysis_, or
    [listwise deletion](https://en.wikipedia.org/wiki/Listwise_deletion) is a
    method for handling missing data by dropping all observations that have any
    missing data. Complete case analysis will yield biased results unless the
    data are MCAR. Complete case analysis also sacrifices precision and
    efficiency.

  - _Single imputation_ is any procedure that replaces each missing value with a
    prediction of it based on available data. The analysis then proceeds as if
    the imputed data were actually observed data. Single imputation can yield
    biased point estimates and nearly always leads to understatement of the
    uncertainty.

  - [Multiple imputation](https://en.wikipedia.org/wiki/Multiple_imputation) is
    a framework in which multiple datasets are created, each one of which has
    the missing data imputed randomly from a distribution that correctly
    reflects its conditional mean and variance, i.e. from
    $P(X_{\rm miss} | X_{\rm obs})$. The analysis is them conducted
    independently on each imputed data set, and the point estimates and sampling
    variances for each imputed data set are pooled to a single point estimate
    and single sampling variance using a _combining rule_.

  - _Full information maximum likelihood (FIML)_ is an approach in which a
    likelihood for the "complete data" is marginalized to a likelihood for the
    observed data. This _observed data likelihood_ is then used for
    maximum-likelihood estimation (or some other likelihood-based estimation
    procedure). This _FIML_ approach can be statistically and computationally
    efficient, and gives unbiased point estimates and uncertainty assessments
    under MCAR and MAR.

  - MNAR (missing not at random) is usually the most difficult situation, and
    generally requires some "side information" about the structure of any
    informative missingness.

  - There are many other ways to handle missing data including many Bayesian
    approaches.

## Methods for observational data

### Paired differences (pre/post)

- A very common study design involves assessing units at baseline and then again
  at a fixed follow-up time. This may be called a _pre-post_ design, and can be
  analyzed using a _paired difference_ approach such as a paired t-test, or with
  a regression approach.

- Let $a_i$ and $b_i$ denote the baseline ("pre") and follow-up ("post")
  measurements for the $i^{\rm th}$ subject, where $i=1, \ldots, n$. One natural
  approach is to take the differences $d_i = b_i - a_i$ and assess the null
  hypothesis that $E[d] = 0$.

- Since ${\rm Var}(d) = {\rm Var}(a) + {\rm Var}(b) - 2{\rm Cov}(a, b)$, if $a$
  and $b$ are positively correlated, the variance of $d$ is less than the sum of
  the variance of $a$ and the variance of $b$. This is because sources of
  variation that are shared between the baseline and follow-up measures cancel
  when taking the difference. For this reason, a pre/post design is usually more
  powerful than a design involving two independent cohorts.

- A more flexible alternative to a paired difference analysis is a regression
  analysis in which the linear model $E[b] = \beta_0 + \beta_1 a$ is fit. The
  value of $\beta_0$ captures the systematic change from baseline to follow-up,
  and the term $\beta_1 a$ adjusts for baseline severity. If $\beta_1 \approx 1$
  then this is equivalent to the paired difference analysis, but if
  $\beta_1 < 1$ we will have more power using linear regression than using the
  paired difference analysis.

### Difference in difference

- Suppose we have two independent cohorts, say a cohort treated with a novel
  treatment and a cohort treated with the standard of care. Each cohort is
  assessed at baseline (prior to any intervention) and at a follow-up time.

- Let $a^c_i$ and $b^c_i$ denote the baseline and follow-up measurements for
  subjects in the control arm and let $a^t_i$ and $b^t_i$ denote the baseline
  and follow-up measurements for subjects in the treated arm. Let
  $d^c_i = b^c_i - a^c_i$ and $d^t_i = b^t_i - a^t_i$ denote the within-subject
  differences for the $i^{\rm th}$ treated and $i^{\rm th}$ control subjects,
  respectively (these are different people). We can compare the two average
  differences, $\bar{d}^c - \bar{d}^t$, yielding a "difference in difference" or
  "diff in diff" analysis.

- In a diff in diff analysis, the "inner differences" $b^c_i - a^c_i$ and
  $b^t_i - a^t_i$ account for any _stable confounders_ that vary little within
  subjects over time. The "outer difference" $\bar{d}^t - \bar{d}^c$ accounts
  for factors that affect the population of interest irrespective of treatment.

- Diff-in-diff is a powerful paradigm that has many variations beyond what we
  discuss here.

### Stratification

- In many studies, the population of interest is heterogeneous and can be
  divided into more homogeneous groups known as _strata_. The strata are defined
  in terms of known and measured variables (potential confounders). Common
  stratifying factors are geography (zip code or county of residence),
  demographics (age, sex, race), or social factors such as employment status or
  cohabitation status.

- The most basic way to conduct a stratified analysis is to compute the
  parameter of interest separately within each stratum, and then to pool these
  stratum-level estimates into an overall estimate. This approach is robust to
  confounding by factors that are (approximately) constant within strata.
  Stratification leads to reduced power, but the loss of power is often small or
  moderate. The loss of power is less if there are fewer strata and the sample
  size per stratum is larger.

- If confounding is present, the specific bias can take different forms. A
  common situation is that the treatment effect estimate is _attenuated_
  (shifted toward the null value) when comparing the estimate obtained using
  stratification to the naive estimate. In some cases the effect may be
  statistically significant in the non-stratified analysis but lose significance
  in the stratified analysis. The opposite can happen as well - the evidence
  against the null becomes stronger following stratification. It is even
  possible that the effect can change sign (e.g. a statistically significant
  positive association can become statistically significant and negative after
  stratification).

### Regression adjustment

- [Regression analysis](https://en.wikipedia.org/wiki/Regression_analysis) is a
  broad class of techniques that can be used to relate the value of an _outcome_
  to the values of one or more _explanatory variables_. Regression analysis is
  commonly used to assess the effect of a treatment or exposure while accounting
  for possible confounders. The most basic form of regression analysis is
  _linear regression_, using _ordinary least squares_ to fit the models to data
  (i.e. to estimate the model parameters).

- A basic example is the linear model
  $E[Y|X,Z] = \beta_0 + \beta_1 X + \beta_2 X$, where $Y$ is the outcome, $X$ is
  the treatment or exposure, and $Z$ is a potential confounder. Under certain
  rather strong assumptions, an estimate of the coefficient $\beta_1$ can be
  used to assess the relationship between the exposure $X$ and the outcome $Y$,
  while controlling for the confounder $Z$.

- Although linear models fit using OLS are widely used in research, this method
  is not usually considered to be the most rigorous way to account for
  confounding. It involves strong structural assumptions about the relationships
  among $X$, $Z$, and $Y$.

- With sufficient data, some alternatives to linear regression can be used to be
  more broadly effective at identifying treatment effects. These methods
  resemble linear regression at a high level but introduce specific
  modifications to address known weaknesses of the method.

  - Flexible regression models can be fit using linear least squares, for
    example using basis functions and interactions.

  - Methods that account for _mean/variance relationships_ and non-linear link
    functions can be used, while maintaining the simplicity of a _single index_.
    [Generalized Linear Models (GLMs)](https://en.wikipedia.org/wiki/Generalized_linear_model)
    are an important class of such methods.

  - Methods that focus on alternative estimation targets (other than the
    conditional mean) such as
    [quantile regression](https://en.wikipedia.org/wiki/Quantile_regression) may
    have better power and/or perform well under weaker assumptions.

  - Methods that exploit known structure such as sparsity or low-dimensionality
    can be used. If the presumed structure is present, these methods can
    accommodate a much greater number of potential confounders (i.e.
    _high-dimensional_ confounding) compared to methods such as ordinary least
    squares.

### Natural experiments

- [Natural experiments](https://en.wikipedia.org/wiki/Natural_experiment) are
  observational studies that exploit incidental circumstances that resemble the
  conditions of an experiment (i.e. that resemble random assignment). They are
  sometimes referred to as _quasi-experimental_ studies.

- Many natural experiments arise due to the staggered timing of events leading
  to different units being exposed at different times. Numerous studies
  examining the impact of media exposure (television, internet, etc.) have been
  conducted that attempt to exploit the staggered construction of networks. For
  example, when cable television was being introduced to the US during the
  1980s, some counties gained access to cable TV before others. One may argue
  that this is a natural experiment since the factors that led to earlier
  introduction of cable services may be primarily driven by logistical factors
  so that the people who gained access to cable TV in, say, 1984 may not be
  systematically different from those who gained access to cable TV in 1985.
  However this can be contested since geographic factors and the degree of
  urbanicity may influence choices about network investments and there certainly
  are differences between people based on these characteristics.

- [Regression discontinuity](https://en.wikipedia.org/wiki/Regression_discontinuity_design)
  is a quasi-experimental strategy that exploits the use of thresholds for
  determining exposures. Here is an example:

  - Suppose that a blood pressure medication is recommended to anyone whose
    systolic blood pressure (SBP) falls over a threshold, say 130 mm Hg. We have
    access to a follow-up SBP measurement taken one year after the treatments
    are assigned. We should not compare the SBP values for all treated people to
    all untreated people, since these two groups were highly unbalanced for
    baseline SBP (the treated people have systematically higher SBP at
    baseline). However, we can consider only the subset of people with, say,
    baseline SBP equal to 129 compared to the subset with baseline SBP equal to
    130\. Given the inherent measurement error in a single SBP reading, we might
    argue that for people with baseline SBP between 129 and 130, the treatment
    assignment is effectively random.

### Matching

- Matching is a strategy that aims to estimate treatment effects (unbiasedly and
  consistently) while accounting for observed confounders.

- Matching aims to yield meaningful results under weak assumptions. The tradeoff
  for achieving this robustness is usually that the statistical power is less
  for matching compared to, say, regression adjustment.

- A simple matching analysis is one in which we have an outcome $y_i$ on units
  indexed by $i=1,2,\ldots,n$, and for each unit we have an exposure indicator
  $x_i \in \\{0, 1\\}$, and covariates $z_i$. The goal of 1:1 matching is to
  identify a set of pairs $1 \le j_{1i}, j_{2i} \le n$ such that subject
  $j_{1i}$ is exposed, i.e. $x_{j_{1i}}=1$, subject $j_{2i}$ is not exposed,
  i.e. $x_{j_{2i}}=0$, and subjects $j_{1i}$ and $j_{2i}$ are similar in terms
  of the covariates, i.e. $z_{j_{1i}}\approx z_{j_{2i}}$.

- As a consequence of the matching, the matched treated subjects should be
  approximately balanced with respect to the matched untreated subjects, i.e.
  for each covariate $\ell$,
  $\bar{z_{j_{1\cdot},\ell}$
  $\bar{z_{j_{1\cdot},\ell} \approx \bar{z_{j_{2\cdot},\ell}}$.

- Matching requires the treated and untreated subjects to have a _common support_
  in the domain of the covariates. For example, consider age as a balancing
  covariate in a study using matching, with 50 exposed units and 50 unexposed
  units. If the treated units are much older than the untreated units, and only
  5 of the treated units are younger than the oldest untreated unit, then there
  is very little common support and matching is unlikely to be effective.

- In a matching analysis, some of the controls may be unused, and some of the
  controls may be used in multiple matched sets. This can lead to reduced power,
  but is unavoidable if the distributions of certain covariates have only a
  minimal common support.

- The term _caliper_ is used to refer to constraints on how the matching is
  done. For example, we may impose a caliper that requires a matched treated and
  control subject to have ages that differ by no more than 5 years.

### Propensity scores

- Suppose we have a situation where treatments are assigned non-randomly. Let
  $t_i \in \\{0, 1\\}$ denote the treatment arm for subject $i$, let $y_i$
  denote the outcome for subject $i$, and let $x_i$ denote covariates for
  subject $i$. For simplicity, we take here the treatment to have two levels,
  but this can easily be generalized.

- The
  [propensity score](https://en.wikipedia.org/wiki/Propensity_score_matching) is
  the probability of being assigned to treatment arm $1$ given the subject
  characteristics, which we can write $p_i = P(t_i = 1 | x_i)$. Since $t_i$ is
  binary here, we can use logistic regression to estimate these probabilities.

- Once we have propensity scores, there are several things that we can do with
  them.

  - Observations can be matched on the propensity scores, e.g. we can seek pairs
    of observations $i_k, j_k$ such that $p_{i_k} \approx p_{j_k}$, $t_{i_k}=1$,
    $t_{j_k} = 0$. These are two units that had the same propensity to be
    treated, but only one of them was actually treated.

  - Stratified analyses can be conducted in which we partition the sample into,
    say, 5 strata based on the propensity scores, estimate treatment effects
    within each stratum, and then pool the results to yield an overall estimate.

  - Propensity scores can be included in regression models as covariates.

  - Propensity scores can be used as weights in
    [inverse probability weighting](https://en.wikipedia.org/wiki/Inverse_probability_weighting)

### Synthetic controls

- A [synthetic control](https://en.wikipedia.org/wiki/Synthetic_control_method)
  is a calculated value for each treated unit that estimates the
  "counterfactual" value that the unit would have had had it not been treated.

- Let $x^\star$ denote the $p$-dimensional vector of covariates associated with a
  treated unit and let $X$ denote the $m\times p$ matrix of covariate values for
  all units assigned to the control arm (in many cases only untreated units with
  covariates "sufficiently similar" to $x^\star$ are included). The goal is to
  construct a vector $w$ such that $x^* \approx X^\prime w$, under the
  constraint that $w^\prime 1_m = 1$ and $0 \le w_j \le 1$ for $j=1, \ldots, m$.
  That is, we approximate $x^\star$ with a
  [convex combination](https://en.wikipedia.org/wiki/Convex_combination) of the
  untreated units.

- The construction of the weights $w$ is a
  [quadratic program](https://en.wikipedia.org/wiki/Quadratic_programming) with
  linear constraints and can be handled with standard solvers.

- Once the weight vector $w$ is constructed, we estimate the counterfactual
  control value $y^*$ using $w^\prime y$, where $y$ is the $m\times 1$ vector
  containing the response values for the controls in $X$.

- Synthetic controls are related to a technique in which (local) linear
  regression is used to estimate the counterfactual control value corresponding
  to each treated unit. However when using linear regression, the weights $w^*$
  can be negative, and therefore the synthetic control may not lie within the
  domain of the data (i.e. it may be an extrapolation).
